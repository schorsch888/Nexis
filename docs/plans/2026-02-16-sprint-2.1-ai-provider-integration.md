# Sprint 2.1: AI Provider 集成 实现计划

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** 实现真实的 AI Provider 集成，支持 OpenAI 和 Anthropic 的 API 调用，包括流式响应、错误重试和健康检查。

**Architecture:** 
- 基于 HttpJsonProvider 扩展，实现 Provider trait
- 每个 Provider 独立模块，遵循 Adapter 模式
- Provider Registry 统一管理，支持动态注册和健康检查
- 使用 rustls-tls 避免 OpenSSL 依赖

**Tech Stack:** Rust 1.75+, reqwest (rustls), serde, async-trait, tokio

---

## 前置准备

**工作目录：** `/home/openclaw/git/Nexis`

**依赖检查：**
```bash
# 确认 Rust 版本
rustc --version  # 需要 >= 1.75.0

# 确认项目编译
cargo build --workspace
```

**环境变量：**
```bash
export OPENAI_API_KEY="sk-..."  # 用户的 OpenAI API Key
export ANTHROPIC_API_KEY="sk-ant-..."  # 用户的 Anthropic API Key
```

**Git 状态：**
```bash
git status  # 应该在 main 分支，无未提交的修改
```

---

## Task 1: OpenAI Provider 实现

**文件：**
- Create: `crates/nexis-runtime/src/providers/openai.rs`
- Modify: `crates/nexis-runtime/Cargo.toml`
- Modify: `crates/nexis-runtime/src/lib.rs`
- Test: `crates/nexis-runtime/src/providers/openai.rs` (内联测试)

**目标：** 实现完整的 OpenAI API 调用能力，包括 Chat Completions 和 Streaming。

### Step 1.1: 添加依赖

**文件：** `crates/nexis-runtime/Cargo.toml`

**操作：** 添加环境变量读取依赖

```toml
[dependencies]
# ... 现有依赖 ...
dotenvy = "0.15"  # 用于读取 .env 文件（开发环境）
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo build -p nexis-runtime
```

**预期：** 编译成功，依赖下载完成

**提交：**
```bash
git add crates/nexis-runtime/Cargo.toml
git commit -m "chore(runtime): add dotenvy dependency for env config"
```

---

### Step 1.2: 创建 OpenAI Provider 模块结构

**文件：** `crates/nexis-runtime/src/providers/mod.rs`

**操作：** 创建 providers 模块

```rust
//! AI Provider implementations
//!
//! This module contains concrete implementations of the AIProvider trait
//! for various AI services (OpenAI, Anthropic, Gemini, etc.)

pub mod openai;

pub use openai::OpenAIProvider;
```

**文件：** `crates/nexis-runtime/src/providers/openai.rs`

**操作：** 创建基础结构

```rust
//! OpenAI API Provider
//!
//! Implements the AIProvider trait for OpenAI's Chat Completions API
//! with support for streaming responses.

use async_trait::async_trait;
use futures::stream::{Stream, StreamExt};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::env;
use std::pin::Pin;
use std::time::Duration;

use crate::{AIProvider, GenerateRequest, GenerateResponse, ProviderError, ProviderStream, StreamChunk};

const OPENAI_API_BASE: &str = "https://api.openai.com/v1";
const DEFAULT_MODEL: &str = "gpt-4o-mini";
const MAX_RETRIES: u32 = 3;
const RETRY_DELAY_MS: u64 = 200;

/// OpenAI API Provider
pub struct OpenAIProvider {
    client: Client,
    api_key: String,
    base_url: String,
    default_model: String,
}

impl OpenAIProvider {
    /// Create new OpenAI provider from environment variable
    ///
    /// # Panics
    ///
    /// Panics if OPENAI_API_KEY environment variable is not set
    pub fn from_env() -> Self {
        let api_key = env::var("OPENAI_API_KEY")
            .expect("OPENAI_API_KEY environment variable must be set");
        
        let base_url = env::var("OPENAI_API_BASE")
            .unwrap_or_else(|_| OPENAI_API_BASE.to_string());
        
        let default_model = env::var("OPENAI_DEFAULT_MODEL")
            .unwrap_or_else(|_| DEFAULT_MODEL.to_string());
        
        Self::new(api_key, base_url, default_model)
    }
    
    /// Create new OpenAI provider with explicit configuration
    pub fn new(api_key: impl Into<String>, base_url: impl Into<String>, default_model: impl Into<String>) -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(60))
            .build()
            .expect("Failed to create HTTP client");
        
        Self {
            client,
            api_key: api_key.into(),
            base_url: base_url.into(),
            default_model: default_model.into(),
        }
    }
    
    /// Build the full API endpoint URL
    fn endpoint(&self, path: &str) -> String {
        format!("{}{}", self.base_url.trim_end_matches('/'), path)
    }
    
    /// Get the model to use (from request or default)
    fn get_model(&self, req: &GenerateRequest) -> String {
        req.model.clone().unwrap_or_else(|| self.default_model.clone())
    }
}

#[async_trait]
impl AIProvider for OpenAIProvider {
    fn name(&self) -> &'static str {
        "openai"
    }
    
    async fn generate(&self, req: GenerateRequest) -> Result<GenerateResponse, ProviderError> {
        // TODO: Implement in next step
        todo!("Implement generate")
    }
    
    async fn generate_stream(&self, req: GenerateRequest) -> Result<ProviderStream, ProviderError> {
        // TODO: Implement in later step
        todo!("Implement generate_stream")
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn provider_creation_from_env() {
        // This test will fail if OPENAI_API_KEY is not set
        // That's OK - it's meant for integration testing
        if env::var("OPENAI_API_KEY").is_ok() {
            let provider = OpenAIProvider::from_env();
            assert_eq!(provider.name(), "openai");
        }
    }
    
    #[test]
    fn provider_creation_explicit() {
        let provider = OpenAIProvider::new(
            "test-key",
            "https://api.example.com/v1",
            "gpt-4"
        );
        assert_eq!(provider.name(), "openai");
        assert_eq!(provider.default_model, "gpt-4");
    }
    
    #[test]
    fn endpoint_building() {
        let provider = OpenAIProvider::new("key", "https://api.openai.com/v1", "gpt-4");
        assert_eq!(provider.endpoint("/chat/completions"), "https://api.openai.com/v1/chat/completions");
        
        let provider2 = OpenAIProvider::new("key", "https://api.openai.com/v1/", "gpt-4");
        assert_eq!(provider2.endpoint("/chat/completions"), "https://api.openai.com/v1/chat/completions");
    }
}
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo test -p nexis-runtime --lib providers::openai::tests
```

**预期：** 3 个测试全部通过

**提交：**
```bash
git add crates/nexis-runtime/src/providers/
git commit -m "feat(runtime): add OpenAI provider module structure

- Create providers module
- Implement OpenAIProvider struct
- Add configuration from env or explicit
- Add basic unit tests"
```

---

### Step 1.3: 实现 OpenAI API 请求/响应类型

**文件：** `crates/nexis-runtime/src/providers/openai.rs`

**操作：** 在文件末尾（tests 模块之前）添加 API 类型定义

```rust
// ============================================================================
// OpenAI API Types
// ============================================================================

/// OpenAI Chat Completion Request
#[derive(Debug, Serialize)]
struct ChatCompletionRequest {
    model: String,
    messages: Vec<Message>,
    #[serde(skip_serializing_if = "Option::is_none")]
    max_tokens: Option<u32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    temperature: Option<f32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    stream: Option<bool>,
}

/// OpenAI Message
#[derive(Debug, Serialize, Deserialize, Clone)]
struct Message {
    role: String,
    content: String,
}

/// OpenAI Chat Completion Response
#[derive(Debug, Deserialize)]
struct ChatCompletionResponse {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<Choice>,
    usage: Option<Usage>,
}

/// OpenAI Choice
#[derive(Debug, Deserialize)]
struct Choice {
    index: u32,
    message: Message,
    finish_reason: Option<String>,
}

/// OpenAI Usage
#[derive(Debug, Deserialize)]
struct Usage {
    prompt_tokens: u32,
    completion_tokens: u32,
    total_tokens: u32,
}

/// OpenAI Streaming Response (SSE event)
#[derive(Debug, Deserialize)]
struct ChatCompletionChunk {
    id: String,
    object: String,
    created: u64,
    model: String,
    choices: Vec<StreamChoice>,
}

/// OpenAI Stream Choice
#[derive(Debug, Deserialize)]
struct StreamChoice {
    index: u32,
    delta: Delta,
    finish_reason: Option<String>,
}

/// OpenAI Delta (partial message)
#[derive(Debug, Deserialize)]
struct Delta {
    #[serde(default)]
    role: Option<String>,
    #[serde(default)]
    content: Option<String>,
}

#[cfg(test)]
mod tests {
    use super::*;
    
    // ... 之前的 tests ...
    
    #[test]
    fn chat_completion_request_serialization() {
        let req = ChatCompletionRequest {
            model: "gpt-4".to_string(),
            messages: vec![
                Message {
                    role: "user".to_string(),
                    content: "Hello".to_string(),
                }
            ],
            max_tokens: Some(100),
            temperature: Some(0.7),
            stream: None,
        };
        
        let json = serde_json::to_string(&req).unwrap();
        assert!(json.contains("\"model\":\"gpt-4\""));
        assert!(json.contains("\"max_tokens\":100"));
        assert!(json.contains("\"temperature\":0.7"));
        assert!(!json.contains("\"stream\"")); // skip_serializing_if works
    }
    
    #[test]
    fn chat_completion_response_deserialization() {
        let json = r#"{
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-4",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "Hello there!"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 10,
                "completion_tokens": 5,
                "total_tokens": 15
            }
        }"#;
        
        let resp: ChatCompletionResponse = serde_json::from_str(json).unwrap();
        assert_eq!(resp.id, "chatcmpl-123");
        assert_eq!(resp.choices.len(), 1);
        assert_eq!(resp.choices[0].message.content, "Hello there!");
        assert_eq!(resp.usage.unwrap().total_tokens, 15);
    }
}
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo test -p nexis-runtime --lib providers::openai::tests
```

**预期：** 5 个测试全部通过（3 + 2 新增）

**提交：**
```bash
git add crates/nexis-runtime/src/providers/openai.rs
git commit -m "feat(runtime): add OpenAI API request/response types

- Define ChatCompletionRequest/Response structs
- Add Message, Choice, Usage types
- Add streaming types (Chunk, Delta)
- Add serialization tests"
```

---

### Step 1.4: 实现 generate() 方法（非流式）

**文件：** `crates/nexis-runtime/src/providers/openai.rs`

**操作：** 替换 `generate()` 方法的 `todo!()`

```rust
async fn generate(&self, req: GenerateRequest) -> Result<GenerateResponse, ProviderError> {
    let openai_req = ChatCompletionRequest {
        model: self.get_model(&req),
        messages: vec![
            Message {
                role: "user".to_string(),
                content: req.prompt,
            }
        ],
        max_tokens: req.max_tokens,
        temperature: req.temperature,
        stream: None,
    };
    
    let response = self.client
        .post(self.endpoint("/chat/completions"))
        .bearer_auth(&self.api_key)
        .json(&openai_req)
        .send()
        .await
        .map_err(|e| ProviderError::Transport(e.to_string()))?;
    
    let status = response.status();
    if !status.is_success() {
        let body = response.text().await.unwrap_or_else(|_| "<unable to read body>".to_string());
        return Err(ProviderError::HttpStatus {
            status: status.as_u16(),
            body,
        });
    }
    
    let openai_resp: ChatCompletionResponse = response
        .json()
        .await
        .map_err(|e| ProviderError::Decode(e.to_string()))?;
    
    // Extract the assistant's message
    let content = openai_resp
        .choices
        .first()
        .map(|c| c.message.content.clone())
        .ok_or_else(|| ProviderError::Decode("No choices in response".to_string()))?;
    
    Ok(GenerateResponse {
        content,
        model: Some(openai_resp.model),
        finish_reason: openai_resp.choices.first().and_then(|c| c.finish_reason.clone()),
    })
}
```

**测试：** 需要实际的 API Key，所以用 Mock Server

**文件：** `crates/nexis-runtime/src/providers/openai.rs` (在 tests 模块中)

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use httpmock::prelude::*;
    
    // ... 之前的 tests ...
    
    #[tokio::test]
    async fn generate_calls_openai_api() {
        let server = MockServer::start();
        
        let mock = server.mock(|when, then| {
            when.method(POST)
                .path("/chat/completions")
                .header("Authorization", "Bearer test-key");
            then.status(200)
                .header("content-type", "application/json")
                .json_body(json!({
                    "id": "chatcmpl-test",
                    "object": "chat.completion",
                    "created": 1234567890,
                    "model": "gpt-4",
                    "choices": [{
                        "index": 0,
                        "message": {
                            "role": "assistant",
                            "content": "Hello! How can I help you?"
                        },
                        "finish_reason": "stop"
                    }],
                    "usage": {
                        "prompt_tokens": 10,
                        "completion_tokens": 20,
                        "total_tokens": 30
                    }
                }));
        });
        
        let provider = OpenAIProvider::new(
            "test-key",
            server.base_url(),
            "gpt-4"
        );
        
        let req = GenerateRequest {
            prompt: "Hello".to_string(),
            model: None,
            max_tokens: Some(100),
            temperature: Some(0.7),
            metadata: None,
        };
        
        let resp = provider.generate(req).await.unwrap();
        
        mock.assert();
        assert_eq!(resp.content, "Hello! How can I help you?");
        assert_eq!(resp.model, Some("gpt-4".to_string()));
        assert_eq!(resp.finish_reason, Some("stop".to_string()));
    }
    
    #[tokio::test]
    async fn generate_handles_api_error() {
        let server = MockServer::start();
        
        server.mock(|when, then| {
            when.method(POST).path("/chat/completions");
            then.status(401)
                .json_body(json!({
                    "error": {
                        "message": "Invalid API key",
                        "type": "invalid_request_error"
                    }
                }));
        });
        
        let provider = OpenAIProvider::new("bad-key", server.base_url(), "gpt-4");
        
        let req = GenerateRequest {
            prompt: "Hello".to_string(),
            model: None,
            max_tokens: None,
            temperature: None,
            metadata: None,
        };
        
        let err = provider.generate(req).await.unwrap_err();
        
        match err {
            ProviderError::HttpStatus { status, .. } => assert_eq!(status, 401),
            _ => panic!("Expected HttpStatus error"),
        }
    }
}
```

**依赖：** 添加 httpmock 到 Cargo.toml

**文件：** `crates/nexis-runtime/Cargo.toml`

```toml
[dev-dependencies]
httpmock = "0.7"
tokio-test = "0.4"
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo test -p nexis-runtime --lib providers::openai::tests::generate
```

**预期：** 2 个新测试通过

**提交：**
```bash
git add crates/nexis-runtime/src/providers/openai.rs crates/nexis-runtime/Cargo.toml
git commit -m "feat(runtime): implement OpenAI generate() method

- Implement non-streaming Chat Completions API call
- Add error handling for HTTP errors
- Add mock server tests for API interaction
- Add httpmock to dev dependencies"
```

---

### Step 1.5: 实现 generate_stream() 方法（流式）

**文件：** `crates/nexis-runtime/src/providers/openai.rs`

**操作：** 替换 `generate_stream()` 方法的 `todo!()`

```rust
async fn generate_stream(&self, req: GenerateRequest) -> Result<ProviderStream, ProviderError> {
    use futures::stream;
    use reqwest_eventsource::{Event, EventSource};
    
    let openai_req = ChatCompletionRequest {
        model: self.get_model(&req),
        messages: vec![
            Message {
                role: "user".to_string(),
                content: req.prompt,
            }
        ],
        max_tokens: req.max_tokens,
        temperature: req.temperature,
        stream: Some(true),
    };
    
    let client = self.client.clone();
    let endpoint = self.endpoint("/chat/completions");
    let api_key = self.api_key.clone();
    
    // Create EventSource for SSE streaming
    let event_source = EventSource::new(
        client
            .post(&endpoint)
            .bearer_auth(&api_key)
            .json(&openai_req)
    )
    .map_err(|e| ProviderError::Transport(e.to_string()))?;
    
    // Convert EventSource to Stream<StreamChunk>
    let stream = event_source
        .take_while(|event| {
            futures::future::ready(!matches!(event, Ok(Event::Message(ref msg)) if msg.data == "[DONE]"))
        })
        .filter_map(|event| async move {
            match event {
                Ok(Event::Message(msg)) => {
                    // Parse the chunk
                    let chunk: Result<ChatCompletionChunk, _> = serde_json::from_str(&msg.data);
                    
                    match chunk {
                        Ok(chunk) => {
                            // Extract text from delta
                            let text = chunk.choices.first()
                                .and_then(|c| c.delta.content.clone())
                                .unwrap_or_default();
                            
                            if text.is_empty() {
                                None
                            } else {
                                Some(Ok(StreamChunk::Delta { text }))
                            }
                        }
                        Err(e) => Some(Err(ProviderError::Decode(e.to_string()))),
                    }
                }
                Ok(Event::Open) => None,
                Err(e) => Some(Err(ProviderError::Transport(e.to_string()))),
            }
        })
        .chain(stream::iter(vec![Ok(StreamChunk::Done)]));
    
    Ok(Box::pin(stream))
}
```

**测试：**

```rust
#[tokio::test]
async fn generate_stream_emits_chunks() {
    let server = MockServer::start();
    
    let mock = server.mock(|when, then| {
        when.method(POST)
            .path("/chat/completions")
            .header("Authorization", "Bearer test-key");
        then.status(200)
            .header("content-type", "text/event-stream")
            .body(concat!(
                "data: {\"id\":\"chatcmpl-test\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\"},\"finish_reason\":null}]}\n\n",
                "data: {\"id\":\"chatcmpl-test\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"Hello\"},\"finish_reason\":null}]}\n\n",
                "data: {\"id\":\"chatcmpl-test\",\"object\":\"chat.completion.chunk\",\"created\":1234567890,\"model\":\"gpt-4\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"!\"},\"finish_reason\":null}]}\n\n",
                "data: [DONE]\n\n"
            ));
    });
    
    let provider = OpenAIProvider::new("test-key", server.base_url(), "gpt-4");
    
    let req = GenerateRequest {
        prompt: "Hi".to_string(),
        model: None,
        max_tokens: None,
        temperature: None,
        metadata: None,
    };
    
    let mut stream = provider.generate_stream(req).await.unwrap();
    
    let mut chunks = vec![];
    while let Some(chunk) = stream.next().await {
        chunks.push(chunk.unwrap());
    }
    
    mock.assert();
    assert_eq!(chunks.len(), 3);
    assert_eq!(chunks[0], StreamChunk::Delta { text: "Hello".to_string() });
    assert_eq!(chunks[1], StreamChunk::Delta { text: "!".to_string() });
    assert_eq!(chunks[2], StreamChunk::Done);
}
```

**依赖：** 添加 reqwest-eventsource

**文件：** `crates/nexis-runtime/Cargo.toml`

```toml
[dependencies]
reqwest-eventsource = "0.6"
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo test -p nexis-runtime --lib providers::openai::tests::generate_stream
```

**预期：** 流式测试通过

**提交：**
```bash
git add crates/nexis-runtime/src/providers/openai.rs crates/nexis-runtime/Cargo.toml
git commit -m "feat(runtime): implement OpenAI generate_stream() method

- Implement streaming Chat Completions with SSE
- Parse delta chunks and emit StreamChunk events
- Add reqwest-eventsource dependency
- Add streaming test with mock server"
```

---

### Step 1.6: 在 lib.rs 中导出 providers

**文件：** `crates/nexis-runtime/src/lib.rs`

**操作：** 在文件顶部添加 providers 模块

```rust
pub mod providers;

// Re-export provider types
pub use providers::{OpenAIProvider};
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo build -p nexis-runtime
```

**预期：** 编译成功，无警告

**提交：**
```bash
git add crates/nexis-runtime/src/lib.rs
git commit -m "feat(runtime): export OpenAIProvider in lib.rs"
```

---

### Step 1.7: 集成测试

**文件：** `tests/integration_openai.rs`

**操作：** 创建集成测试

```rust
//! Integration tests for OpenAI Provider
//!
//! These tests require OPENAI_API_KEY to be set
//! Run with: cargo test --test integration_openai -- --ignored

use nexis_runtime::{AIProvider, GenerateRequest, OpenAIProvider, StreamChunk};
use futures::StreamExt;

#[tokio::test]
#[ignore] // Run with --ignored flag
async fn openai_generate_real_api() {
    let provider = OpenAIProvider::from_env();
    
    let req = GenerateRequest {
        prompt: "Say 'Hello, Nexis!' and nothing else.".to_string(),
        model: Some("gpt-4o-mini".to_string()),
        max_tokens: Some(50),
        temperature: Some(0.0),
        metadata: None,
    };
    
    let resp = provider.generate(req).await.unwrap();
    
    assert!(resp.content.contains("Hello"));
    assert_eq!(resp.model, Some("gpt-4o-mini".to_string()));
    println!("Response: {}", resp.content);
}

#[tokio::test]
#[ignore] // Run with --ignored flag
async fn openai_stream_real_api() {
    let provider = OpenAIProvider::from_env();
    
    let req = GenerateRequest {
        prompt: "Count from 1 to 5, one number per line.".to_string(),
        model: Some("gpt-4o-mini".to_string()),
        max_tokens: Some(50),
        temperature: Some(0.0),
        metadata: None,
    };
    
    let mut stream = provider.generate_stream(req).await.unwrap();
    
    let mut full_text = String::new();
    while let Some(chunk) = stream.next().await {
        match chunk.unwrap() {
            StreamChunk::Delta { text } => {
                full_text.push_str(&text);
                print!("{}", text);
            }
            StreamChunk::Done => break,
        }
    }
    
    println!("\nFull response: {}", full_text);
    assert!(full_text.contains('1'));
    assert!(full_text.contains('5'));
}
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
# 单元测试
cargo test -p nexis-runtime --lib

# 集成测试（需要 API Key）
export OPENAI_API_KEY="sk-..."
cargo test --test integration_openai -- --ignored
```

**预期：** 所有测试通过

**提交：**
```bash
git add tests/integration_openai.rs
git commit -m "test(runtime): add OpenAI integration tests

- Add real API tests (ignored by default)
- Test both generate() and generate_stream()
- Run with --ignored flag when API keys available"
```

---

## Task 2: Anthropic Provider 实现

**文件：**
- Create: `crates/nexis-runtime/src/providers/anthropic.rs`
- Modify: `crates/nexis-runtime/src/providers/mod.rs`
- Test: 内联测试 + `tests/integration_anthropic.rs`

**目标：** 实现 Anthropic Claude API 调用，与 OpenAI Provider 结构类似。

### Step 2.1: 创建 Anthropic Provider 模块

**文件：** `crates/nexis-runtime/src/providers/anthropic.rs`

**操作：** 创建 Anthropic Provider（类似 OpenAI，但适配 Anthropic API）

```rust
//! Anthropic Claude API Provider
//!
//! Implements the AIProvider trait for Anthropic's Messages API
//! with support for streaming responses.

use async_trait::async_trait;
use futures::stream::{Stream, StreamExt};
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::env;
use std::pin::Pin;
use std::time::Duration;

use crate::{AIProvider, GenerateRequest, GenerateResponse, ProviderError, ProviderStream, StreamChunk};

const ANTHROPIC_API_BASE: &str = "https://api.anthropic.com/v1";
const DEFAULT_MODEL: &str = "claude-3-5-sonnet-20241022";
const API_VERSION: &str = "2023-06-01";

/// Anthropic API Provider
pub struct AnthropicProvider {
    client: Client,
    api_key: String,
    base_url: String,
    default_model: String,
}

impl AnthropicProvider {
    /// Create new Anthropic provider from environment variable
    pub fn from_env() -> Self {
        let api_key = env::var("ANTHROPIC_API_KEY")
            .expect("ANTHROPIC_API_KEY environment variable must be set");
        
        let base_url = env::var("ANTHROPIC_API_BASE")
            .unwrap_or_else(|_| ANTHROPIC_API_BASE.to_string());
        
        let default_model = env::var("ANTHROPIC_DEFAULT_MODEL")
            .unwrap_or_else(|_| DEFAULT_MODEL.to_string());
        
        Self::new(api_key, base_url, default_model)
    }
    
    /// Create new Anthropic provider with explicit configuration
    pub fn new(api_key: impl Into<String>, base_url: impl Into<String>, default_model: impl Into<String>) -> Self {
        let client = Client::builder()
            .timeout(Duration::from_secs(60))
            .build()
            .expect("Failed to create HTTP client");
        
        Self {
            client,
            api_key: api_key.into(),
            base_url: base_url.into(),
            default_model: default_model.into(),
        }
    }
    
    fn endpoint(&self, path: &str) -> String {
        format!("{}{}", self.base_url.trim_end_matches('/'), path)
    }
    
    fn get_model(&self, req: &GenerateRequest) -> String {
        req.model.clone().unwrap_or_else(|| self.default_model.clone())
    }
}

// ============================================================================
// Anthropic API Types
// ============================================================================

/// Anthropic Messages Request
#[derive(Debug, Serialize)]
struct MessagesRequest {
    model: String,
    messages: Vec<AnthropicMessage>,
    max_tokens: u32,
    #[serde(skip_serializing_if = "Option::is_none")]
    stream: Option<bool>,
}

/// Anthropic Message
#[derive(Debug, Serialize, Clone)]
struct AnthropicMessage {
    role: String,
    content: String,
}

/// Anthropic Messages Response
#[derive(Debug, Deserialize)]
struct MessagesResponse {
    id: String,
    #[serde(rename = "type")]
    response_type: String,
    role: String,
    content: Vec<ContentBlock>,
    model: String,
    stop_reason: Option<String>,
    usage: Usage,
}

/// Anthropic Content Block
#[derive(Debug, Deserialize)]
struct ContentBlock {
    #[serde(rename = "type")]
    block_type: String,
    text: String,
}

/// Anthropic Usage
#[derive(Debug, Deserialize)]
struct Usage {
    input_tokens: u32,
    output_tokens: u32,
}

/// Anthropic Streaming Event
#[derive(Debug, Deserialize)]
#[serde(tag = "type")]
enum StreamEvent {
    #[serde(rename = "content_block_delta")]
    ContentBlockDelta {
        index: u32,
        delta: DeltaContent,
    },
    #[serde(rename = "message_stop")]
    MessageStop,
}

/// Anthropic Delta Content
#[derive(Debug, Deserialize)]
struct DeltaContent {
    #[serde(rename = "type")]
    delta_type: String,
    text: String,
}

#[async_trait]
impl AIProvider for AnthropicProvider {
    fn name(&self) -> &'static str {
        "anthropic"
    }
    
    async fn generate(&self, req: GenerateRequest) -> Result<GenerateResponse, ProviderError> {
        let anthropic_req = MessagesRequest {
            model: self.get_model(&req),
            messages: vec![
                AnthropicMessage {
                    role: "user".to_string(),
                    content: req.prompt,
                }
            ],
            max_tokens: req.max_tokens.unwrap_or(1024),
            stream: None,
        };
        
        let response = self.client
            .post(self.endpoint("/messages"))
            .header("x-api-key", &self.api_key)
            .header("anthropic-version", API_VERSION)
            .header("content-type", "application/json")
            .json(&anthropic_req)
            .send()
            .await
            .map_err(|e| ProviderError::Transport(e.to_string()))?;
        
        let status = response.status();
        if !status.is_success() {
            let body = response.text().await.unwrap_or_else(|_| "<unable to read body>".to_string());
            return Err(ProviderError::HttpStatus {
                status: status.as_u16(),
                body,
            });
        }
        
        let anthropic_resp: MessagesResponse = response
            .json()
            .await
            .map_err(|e| ProviderError::Decode(e.to_string()))?;
        
        // Extract text from content blocks
        let content = anthropic_resp
            .content
            .iter()
            .filter_map(|block| {
                if block.block_type == "text" {
                    Some(block.text.as_str())
                } else {
                    None
                }
            })
            .collect::<Vec<_>>()
            .join("");
        
        Ok(GenerateResponse {
            content,
            model: Some(anthropic_resp.model),
            finish_reason: anthropic_resp.stop_reason,
        })
    }
    
    async fn generate_stream(&self, req: GenerateRequest) -> Result<ProviderStream, ProviderError> {
        // TODO: Implement streaming for Anthropic
        // Anthropic uses different SSE format than OpenAI
        unimplemented!("Anthropic streaming not yet implemented")
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn provider_creation_explicit() {
        let provider = AnthropicProvider::new(
            "test-key",
            "https://api.anthropic.com/v1",
            "claude-3-opus"
        );
        assert_eq!(provider.name(), "anthropic");
        assert_eq!(provider.default_model, "claude-3-opus");
    }
    
    #[tokio::test]
    async fn generate_calls_anthropic_api() {
        use httpmock::prelude::*;
        
        let server = MockServer::start();
        
        let mock = server.mock(|when, then| {
            when.method(POST)
                .path("/messages")
                .header("x-api-key", "test-key")
                .header("anthropic-version", API_VERSION);
            then.status(200)
                .json_body(json!({
                    "id": "msg_test",
                    "type": "message",
                    "role": "assistant",
                    "content": [{
                        "type": "text",
                        "text": "Hello! I'm Claude."
                    }],
                    "model": "claude-3-5-sonnet-20241022",
                    "stop_reason": "end_turn",
                    "usage": {
                        "input_tokens": 10,
                        "output_tokens": 20
                    }
                }));
        });
        
        let provider = AnthropicProvider::new("test-key", server.base_url(), "claude-3-5-sonnet-20241022");
        
        let req = GenerateRequest {
            prompt: "Hello".to_string(),
            model: None,
            max_tokens: Some(100),
            temperature: None,
            metadata: None,
        };
        
        let resp = provider.generate(req).await.unwrap();
        
        mock.assert();
        assert_eq!(resp.content, "Hello! I'm Claude.");
        assert_eq!(resp.model, Some("claude-3-5-sonnet-20241022".to_string()));
    }
}
```

**文件：** `crates/nexis-runtime/src/providers/mod.rs`

**操作：** 添加 anthropic 模块

```rust
pub mod anthropic;
pub mod openai;

pub use anthropic::AnthropicProvider;
pub use openai::OpenAIProvider;
```

**文件：** `crates/nexis-runtime/src/lib.rs`

**操作：** 导出 AnthropicProvider

```rust
pub use providers::{AnthropicProvider, OpenAIProvider};
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo test -p nexis-runtime --lib providers::anthropic
```

**预期：** 测试通过

**提交：**
```bash
git add crates/nexis-runtime/src/providers/anthropic.rs
git add crates/nexis-runtime/src/providers/mod.rs
git add crates/nexis-runtime/src/lib.rs
git commit -m "feat(runtime): implement Anthropic provider

- Add AnthropicProvider with Messages API support
- Implement generate() method
- Add Anthropic-specific request/response types
- Add mock server tests
- Streaming support to be added later"
```

---

## Task 3: Provider Registry 实现

**文件：**
- Create: `crates/nexis-runtime/src/registry.rs`
- Modify: `crates/nexis-runtime/src/lib.rs`
- Test: 内联测试

**目标：** 实现统一的 Provider 注册中心，支持动态注册、健康检查和默认 Provider。

### Step 3.1: 创建 Provider Registry

**文件：** `crates/nexis-runtime/src/registry.rs`

```rust
//! AI Provider Registry
//!
//! Central registry for managing AI providers with support for
//! dynamic registration, health checks, and default provider selection.

use std::collections::HashMap;
use std::sync::Arc;
use tokio::sync::RwLock;

use crate::{AIProvider, ProviderError};

/// Provider registry for managing multiple AI providers
pub struct ProviderRegistry {
    providers: RwLock<HashMap<String, Arc<dyn AIProvider>>>,
    default_provider: RwLock<Option<String>>,
}

impl ProviderRegistry {
    /// Create a new empty registry
    pub fn new() -> Self {
        Self {
            providers: RwLock::new(HashMap::new()),
            default_provider: RwLock::new(None),
        }
    }
    
    /// Register a provider
    pub async fn register(&self, name: impl Into<String>, provider: Arc<dyn AIProvider>) {
        let name = name.into();
        let mut providers = self.providers.write().await;
        
        // Set as default if this is the first provider
        if providers.is_empty() {
            let mut default = self.default_provider.write().await;
            *default = Some(name.clone());
        }
        
        providers.insert(name, provider);
    }
    
    /// Get a provider by name
    pub async fn get(&self, name: &str) -> Option<Arc<dyn AIProvider>> {
        let providers = self.providers.read().await;
        providers.get(name).cloned()
    }
    
    /// Get the default provider
    pub async fn get_default(&self) -> Option<Arc<dyn AIProvider>> {
        let default = self.default_provider.read().await;
        if let Some(name) = default.as_ref() {
            self.get(name).await
        } else {
            None
        }
    }
    
    /// Set the default provider
    pub async fn set_default(&self, name: &str) -> Result<(), ProviderError> {
        let providers = self.providers.read().await;
        
        if !providers.contains_key(name) {
            return Err(ProviderError::Message(format!("Provider '{}' not found", name)));
        }
        
        let mut default = self.default_provider.write().await;
        *default = Some(name.to_string());
        
        Ok(())
    }
    
    /// List all registered providers
    pub async fn list(&self) -> Vec<String> {
        let providers = self.providers.read().await;
        providers.keys().cloned().collect()
    }
    
    /// Check health of all providers
    pub async fn health_check(&self) -> HashMap<String, bool> {
        let providers = self.providers.read().await;
        let mut results = HashMap::new();
        
        for (name, provider) in providers.iter() {
            // Simple health check: try to get the provider name
            // In production, you might want to make a lightweight API call
            let healthy = std::sync::Arc::strong_count(provider) > 0;
            results.insert(name.clone(), healthy);
        }
        
        results
    }
}

impl Default for ProviderRegistry {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{GenerateRequest, GenerateResponse, ProviderStream};
    use async_trait::async_trait;
    
    struct MockProvider {
        name: &'static str,
    }
    
    #[async_trait]
    impl AIProvider for MockProvider {
        fn name(&self) -> &'static str {
            self.name
        }
        
        async fn generate(&self, _req: GenerateRequest) -> Result<GenerateResponse, ProviderError> {
            Ok(GenerateResponse {
                content: "mock response".to_string(),
                model: Some("mock".to_string()),
                finish_reason: None,
            })
        }
        
        async fn generate_stream(&self, _req: GenerateRequest) -> Result<ProviderStream, ProviderError> {
            unimplemented!()
        }
    }
    
    #[tokio::test]
    async fn register_and_get_provider() {
        let registry = ProviderRegistry::new();
        let provider = Arc::new(MockProvider { name: "test" });
        
        registry.register("test", provider).await;
        
        let retrieved = registry.get("test").await;
        assert!(retrieved.is_some());
        assert_eq!(retrieved.unwrap().name(), "test");
    }
    
    #[tokio::test]
    async fn default_provider() {
        let registry = ProviderRegistry::new();
        
        // First provider becomes default
        let provider1 = Arc::new(MockProvider { name: "p1" });
        registry.register("p1", provider1).await;
        
        let default = registry.get_default().await;
        assert_eq!(default.unwrap().name(), "p1");
        
        // Add second provider
        let provider2 = Arc::new(MockProvider { name: "p2" });
        registry.register("p2", provider2).await;
        
        // Default should still be p1
        let default = registry.get_default().await;
        assert_eq!(default.unwrap().name(), "p1");
        
        // Change default
        registry.set_default("p2").await.unwrap();
        let default = registry.get_default().await;
        assert_eq!(default.unwrap().name(), "p2");
    }
    
    #[tokio::test]
    async fn list_providers() {
        let registry = ProviderRegistry::new();
        
        let provider1 = Arc::new(MockProvider { name: "p1" });
        let provider2 = Arc::new(MockProvider { name: "p2" });
        
        registry.register("p1", provider1).await;
        registry.register("p2", provider2).await;
        
        let list = registry.list().await;
        assert_eq!(list.len(), 2);
        assert!(list.contains(&"p1".to_string()));
        assert!(list.contains(&"p2".to_string()));
    }
    
    #[tokio::test]
    async fn health_check() {
        let registry = ProviderRegistry::new();
        let provider = Arc::new(MockProvider { name: "test" });
        
        registry.register("test", provider).await;
        
        let health = registry.health_check().await;
        assert_eq!(health.get("test"), Some(&true));
    }
}
```

**文件：** `crates/nexis-runtime/src/lib.rs`

**操作：** 导出 registry

```rust
mod registry;

pub use registry::ProviderRegistry;
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo test -p nexis-runtime --lib registry
```

**预期：** 4 个测试通过

**提交：**
```bash
git add crates/nexis-runtime/src/registry.rs crates/nexis-runtime/src/lib.rs
git commit -m "feat(runtime): implement ProviderRegistry

- Add central registry for managing providers
- Support dynamic registration
- Support default provider selection
- Add health check capability
- Add comprehensive tests"
```

---

## Task 4: CLI 集成测试

**文件：**
- Modify: `crates/nexis-cli/src/main.rs`
- Test: 手动测试

**目标：** 在 CLI 中测试 Provider 调用。

### Step 4.1: 添加 CLI 测试命令

**文件：** `crates/nexis-cli/src/main.rs`

**操作：** 添加 `test-provider` 命令

```rust
use clap::{Parser, Subcommand};
use nexis_runtime::{AIProvider, GenerateRequest, OpenAIProvider, AnthropicProvider, ProviderRegistry};
use std::sync::Arc;

#[derive(Parser)]
#[command(name = "nexis")]
#[command(about = "Nexis AI-Native Team Communication Platform", long_about = None)]
struct Cli {
    #[command(subcommand)]
    command: Commands,
}

#[derive(Subcommand)]
enum Commands {
    /// Test AI provider connection
    TestProvider {
        /// Provider to test (openai or anthropic)
        #[arg(short, long)]
        provider: String,
        
        /// Prompt to send
        #[arg(short, long)]
        prompt: String,
        
        /// Use streaming
        #[arg(short, long)]
        stream: bool,
    },
}

#[tokio::main]
async fn main() -> anyhow::Result<()> {
    let cli = Cli::parse();
    
    match cli.command {
        Commands::TestProvider { provider, prompt, stream } => {
            test_provider(&provider, &prompt, stream).await?;
        }
    }
    
    Ok(())
}

async fn test_provider(provider: &str, prompt: &str, stream: bool) -> anyhow::Result<()> {
    println!("Testing {} provider...", provider);
    
    let provider: Arc<dyn AIProvider> = match provider {
        "openai" => Arc::new(OpenAIProvider::from_env()),
        "anthropic" => Arc::new(AnthropicProvider::from_env()),
        _ => anyhow::bail!("Unknown provider: {}", provider),
    };
    
    let req = GenerateRequest {
        prompt: prompt.to_string(),
        model: None,
        max_tokens: Some(100),
        temperature: Some(0.7),
        metadata: None,
    };
    
    if stream {
        println!("Streaming response:\n");
        use futures::StreamExt;
        let mut stream = provider.generate_stream(req).await?;
        while let Some(chunk) = stream.next().await {
            match chunk? {
                nexis_runtime::StreamChunk::Delta { text } => print!("{}", text),
                nexis_runtime::StreamChunk::Done => println!(),
            }
        }
    } else {
        println!("Sending request...\n");
        let resp = provider.generate(req).await?;
        println!("Response: {}", resp.content);
        println!("Model: {:?}", resp.model);
        println!("Finish reason: {:?}", resp.finish_reason);
    }
    
    Ok(())
}
```

**命令：**
```bash
cd /home/openclaw/git/Nexis
cargo build -p nexis-cli

# 测试 OpenAI
export OPENAI_API_KEY="sk-..."
./target/debug/nexis test-provider --provider openai --prompt "Hello"

# 测试 Anthropic
export ANTHROPIC_API_KEY="sk-ant-..."
./target/debug/nexis test-provider --provider anthropic --prompt "Hello"

# 测试流式
./target/debug/nexis test-provider --provider openai --prompt "Hello" --stream
```

**预期：** 成功调用 API 并返回响应

**提交：**
```bash
git add crates/nexis-cli/src/main.rs
git commit -m "feat(cli): add test-provider command

- Add CLI command to test AI providers
- Support both OpenAI and Anthropic
- Support streaming mode
- Useful for testing provider integration"
```

---

## 完成标准

### 功能完整性

- [ ] OpenAI Provider 实现完整
  - [ ] generate() 方法工作正常
  - [ ] generate_stream() 方法工作正常
  - [ ] 错误处理完整
  - [ ] Mock 测试通过
  - [ ] 集成测试通过

- [ ] Anthropic Provider 实现完整
  - [ ] generate() 方法工作正常
  - [ ] 错误处理完整
  - [ ] Mock 测试通过

- [ ] Provider Registry 实现完整
  - [ ] 注册/获取功能正常
  - [ ] 默认 Provider 设置正常
  - [ ] 健康检查功能正常

- [ ] CLI 集成
  - [ ] test-provider 命令工作正常
  - [ ] 流式输出正常

### 代码质量

- [ ] 所有测试通过：`cargo test --workspace`
- [ ] 无 clippy 警告：`cargo clippy --workspace`
- [ ] 格式正确：`cargo fmt --all -- --check`
- [ ] 无安全漏洞：`cargo audit`
- [ ] 文档完整：所有 public API 有文档注释

### 性能

- [ ] API 调用延迟 < 5s (P95)
- [ ] 流式首字节延迟 < 2s (P95)
- [ ] 无明显内存泄漏

### 文档

- [ ] README 更新（Provider 使用说明）
- [ ] API 文档完整
- [ ] CHANGELOG 更新

---

## 提交历史

预期提交顺序：

1. `chore(runtime): add dotenvy dependency for env config`
2. `feat(runtime): add OpenAI provider module structure`
3. `feat(runtime): add OpenAI API request/response types`
4. `feat(runtime): implement OpenAI generate() method`
5. `feat(runtime): implement OpenAI generate_stream() method`
6. `feat(runtime): export OpenAIProvider in lib.rs`
7. `test(runtime): add OpenAI integration tests`
8. `feat(runtime): implement Anthropic provider`
9. `feat(runtime): implement ProviderRegistry`
10. `feat(cli): add test-provider command`

---

## 后续任务（Sprint 2.2）

完成本 Sprint 后，下一步是：

**Sprint 2.2: 消息持久化**
- PostgreSQL Schema 设计
- SQLx 集成
- Repository 层实现
- 集成测试

---

**计划创建时间：** 2026-02-16  
**计划负责人：** evol (AI PM)  
**预计工期：** 2 周（Sprint 2.1）
